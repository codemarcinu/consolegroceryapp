# 🏠 Asystent Zakupów i Spiżarni v2 - Instrukcja instalacji

## 📋 Wymagania systemowe

### Podstawowe wymagania
- **Python 3.8+** 
- **pip3** (menedżer pakietów Python)
- **4GB RAM** (minimum dla Ollama)
- **2GB wolnego miejsca** na dysku

### Dla funkcji OCR (opcjonalnie)
- **Tesseract OCR** z polskim pakietem językowym
- **OpenCV** dependencies

## 🔧 Instalacja krok po kroku

### 1. Przygotowanie środowiska

**Linux (Ubuntu/Debian):**
```bash
# Aktualizacja systemu
sudo apt update && sudo apt upgrade -y

# Python i narzędzia
sudo apt install python3 python3-pip python3-venv -y

# Tesseract OCR (opcjonalnie - dla funkcji paragonów)
sudo apt install tesseract-ocr tesseract-ocr-pol -y
sudo apt install libopencv-dev python3-opencv -y
sudo apt install libgl1-mesa-glx libglib2.0-0 -y
```

**Windows:**
```cmd
# 1. Pobierz Python 3.8+ z python.org
# 2. Zainstaluj z opcją "Add Python to PATH"
# 3. Tesseract (opcjonalnie):
#    - Pobierz z: https://github.com/UB-Mannheim/tesseract/wiki
#    - Zainstaluj do C:\Program Files\Tesseract-OCR
#    - Dodaj do zmiennej PATH
```

**macOS:**
```bash
# Homebrew (jeśli nie masz)
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Python i narzędzia
brew install python3 python3-pip

# Tesseract (opcjonalnie)
brew install tesseract tesseract-lang
```

### 2. Klonowanie repozytorium

```bash
git clone <REPO_URL>
cd asystent-spizarni
```

### 3. Automatyczna instalacja (Rekomendowane)

**Linux/macOS:**
```bash
chmod +x setup.sh
./setup.sh
```

**Windows:**
```cmd
python -m venv venv
venv\Scripts\activate
pip install --upgrade pip
pip install -r requirements.txt
```

### 4. Instalacja Ollama (AI/LLM)

**Linux/macOS:**
```bash
# Instalacja Ollama
curl https://ollama.ai/install.sh | sh

# Uruchomienie serwera Ollama
ollama serve &

# Pobranie modelu Bielik (polski model AI)
ollama pull speakleash/bielik-1.5b-v3.0-instruct

# Test modelu
ollama run speakleash/bielik-1.5b-v3.0-instruct "Witaj! Jestem asystentem spiżarni."
```

**Windows:**
```cmd
# 1. Pobierz Ollama z: https://ollama.ai/download/windows
# 2. Zainstaluj Ollama
# 3. Otwórz PowerShell/CMD i uruchom:
ollama pull speakleash/bielik-1.5b-v3.0-instruct
ollama serve
```

### 5. Uruchomienie aplikacji

```bash
# Aktywuj środowisko wirtualne (jeśli używasz)
source venv/bin/activate  # Linux/macOS
# lub
venv\Scripts\activate     # Windows

# Uruchom aplikację
python main.py
```

## 🚀 Pierwsze uruchomienie

### Struktura folderów (tworzona automatycznie):
```
asystent-spizarni/
├── data/              # Baza danych produktów
├── paragony/
│   ├── nowe/         # Tutaj wrzucaj nowe paragony (zdjęcia)
│   ├── przetworzone/ # Przetworzone paragony
│   └── bledy/        # Paragony z błędami OCR
└── ...
```

### Konfiguracja aplikacji:
- Plik `data/config.json` jest tworzony automatycznie przy pierwszym uruchomieniu
- Można modyfikować ustawienia LLM, OCR, interfejsu

## 📱 Workflow użytkowania

### 1. Dodawanie produktów ręcznie
- Opcja 1: Dodaj produkt (szybko)
- AI automatycznie sugeruje kategorię i datę ważności

### 2. Automatyczne przetwarzanie paragonów
```
Zrób zdjęcie paragonu → Skopiuj do paragony/nowe/ → 
Opcja 2: Przetwórz paragony → Opcja 3: Importuj do spiżarni
```

### 3. Codzienne zarządzanie
- **Rano**: Sprawdź wygasające produkty (automatycznie przy starcie)
- **Podczas gotowania**: Opcja 5 - Szybko znajdź i oznacz produkty
- **Planowanie**: Opcja 6 - AI podpowie przepisy na podstawie spiżarni

## 🔧 Rozwiązywanie problemów

### Problem: "Błąd połączenia z LLM Ollama"
**Rozwiązanie:**
```bash
# Sprawdź czy Ollama działa
ollama list
curl http://localhost:11434/api/tags

# Jeśli nie działa, uruchom ponownie
ollama serve &
```

### Problem: "EasyOCR nie rozpoznaje tekstu"
**Rozwiązanie:**
- Sprawdź jakość zdjęcia paragonu
- Lepsze oświetlenie, płaska powierzchnia
- Ponownie zrób zdjęcie

### Problem: "ModuleNotFoundError: No module named 'X'"
**Rozwiązanie:**
```bash
# Sprawdź czy środowisko wirtualne jest aktywne
source venv/bin/activate

# Zainstaluj brakujące pakiety
pip install -r requirements.txt
```

### Problem: Aplikacja działa wolno
**Rozwiązanie:**
- **GPU**: Ustaw `"gpu": true` w `data/config.json` (jeśli masz GPU)
- **CPU**: Model Bielik 1.5B jest zoptymalizowany pod CPU
- **RAM**: Zamknij niepotrzebne aplikacje

## 📝 Funkcje bez LLM

Jeśli masz problemy z Ollama, aplikacja nadal działa z ograniczonymi funkcjami:
- ✅ Dodawanie produktów (ręczne kategoryzowanie)
- ✅ Przeglądanie spiżarni 
- ✅ Zarządzanie produktami
- ✅ Statystyki
- ❌ Automatyczne kategoryzowanie
- ❌ Sugerowanie dat ważności
- ❌ Sugestie przepisów
- ❌ Parsowanie paragonów

## 🆘 Wsparcie

1. **Sprawdź logi** - aplikacja wypisuje szczegółowe błędy
2. **Sprawdź konfigurację** - `data/config.json`
3. **Sprawdź serwer Ollama** - `curl http://localhost:11434/api/tags`
4. **GitHub Issues** - zgłoś problem w repozytorium

## 🎯 Wskazówki dla najlepszych rezultatów

### Jakość zdjęć paragonów:
- **Dobre oświetlenie** (bez cieni)
- **Płaska powierzchnia** (bez zagięć)  
- **Wyraźny tekst** (nie rozmazany)
- **Cały paragon** w kadrze

### Optymalizacja wydajności:
- Wyłącz niepotrzebne aplikacje podczas przetwarzania OCR
- Używaj modelu Bielik 1.5B (już w konfiguracji)
- Regularnie archiwizuj stare produkty

### Codzienne użycie:
- **Rano**: Sprawdź komunikaty o wygasających produktach
- **Po zakupach**: Dodaj paragon do folderu nowe/
- **Podczas gotowania**: Użyj wyszukiwania produktów (Opcja 5)
- **Planowanie**: Generuj przepisy (Opcja 6)